<!Doctype html>
<html lang="en">
    <head>
        <title>EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Burak Ercan">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" type="image/png" href="data/bunny.png"/>

        <link rel="stylesheet" type="text/css" href="style_project_page.css?cache=7754391418498779889">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="stylesheet" href="https://unpkg.com/@glidejs/glide/dist/css/glide.core.min.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://unpkg.com/@glidejs/glide"></script>
        <style type="text/css">
            .side-text {
                width:60%;
                display:inline-block;
                vertical-align:top;
            }
            .side-image {
                width: 38%;
                display: inline-block;
                vertical-align: top;
            }
            .controls {
                margin-bottom: 10px;
            }
            .left-controls {
                display: inline-block;
                vertical-align: top;
                width: 80%;
            }
            .right-controls {
                display: inline-block;
                vertical-align: top;
                width: 19%;
                text-align: right;
            }
            .render_window {
                display: inline-block;
                vertical-align: middle;
                box-shadow: 1px 0px 5px black;
                margin-right: 10px;
                margin-bottom: 10px;
                width: calc(33% - 10px);
            }
            .progress {
                background: #666;
                position: relative;
                height: 5px;
                margin-bottom: -5px;
                display: none;
            }
            .glide__slide:hover {cursor: grab;}
            .glide__slide:active {cursor: grabbing;}
            .glide__slide img {width: 90%;}
            .glide__bullets {
                text-align: center;
            }
            .glide__bullet--active {
                color: #aaa; 
            }

            @media (max-width: 400px) {
                .render_window {
                    display: block;
                    width: 90%;
                    margin: 10px auto;
                }
            }
            @media (max-width: 700px) {
                .side-image {
                    display: block;
                    width: 80%;
                    margin: 10px auto;
                }
                .side-text {
                    display: block;
                    width: 100%;
                }
            }
        </style>
    </head>
    <body>
        <div class="section">
            <h1 class="project-title">
                EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction
            </h1>
            <div class="authors">
                <a href=https://ercanburak.github.io/>
                    Burak Ercan <sup>1,2</sup>
                </a>
                <a href=https://github.com/ekeronur/>
                    Onur Eker <sup>1,2</sup>
                </a>
                <a href=https://aykuterdem.github.io/>
                    Aykut Erdem <sup>3,4</sup>
                </a>
                <a href=https://web.cs.hacettepe.edu.tr/~erkut/>
                    Erkut Erdem <sup>1,4</sup>
                </a>
            </div>

            <div class="affiliations">           
                <span><sup>1</sup> <a href=https://cs.hacettepe.edu.tr> Hacettepe University, Computer Engineering Department</a>   </span>
                <span><sup>2</sup> <a href=https://www.havelsan.com.tr/en> HAVELSAN Inc.</a></span> <br/>
                <span><sup>3</sup> <a href=https://cs.ku.edu.tr> Koç University, Computer Engineering Department</a></span>
                <span><sup>4</sup> <a href=https://ai.ku.edu.tr> Koç University, KUIS AI Center</a></span> <br/>
            </div>

            <div class="project-conference">
                <a href=https://tub-rip.github.io/eventvision2023/>CVPR 2023 Workshop on Event-based Vision</a>                
            </div>

            <div class="project-icons">
                <a href="https://arxiv.org/abs/2305.00434/">
                    <i class="fa fa-file"></i> <br/>
                    Paper
                </a>
                <a href="https://github.com/ercanburak/EVREAL">
                    <i class="fa fa-github"></i> <br/>
                    Code
                </a>
            </div>

            <div class="teaser-image">
                <p class="content">Here we present <strong>EVREAL - Event-based Video Reconstruction Evaluation and Analysis Library. </strong> Our open-source framework offers a unified evaluation pipeline to comprehensively benchmark PyTorch based <strong>pre-trained neural networks</strong> for <strong>event-based video reconstruction</strong>, and a result analysis tool to <strong>visualize and compare</strong> reconstructions and their scores. <br/> </p>
                <img src="projects/evreal/diagram.png" style="width:100%;">
                <p class="content"> <br/>  We use a large set of real-world test sequences and various full-reference and no-reference image quality metrics to perform qualitative and quantitative analysis under diverse conditions, including challenging scenarios such as rapid motion, low light, and high dynamic range. Furthermore, we conduct additional experiments to assess the performance of each method under variable conditions and analyze their robustness to these varying settings, including event rate, event tensor sparsity, reconstruction rate, and temporal irregularity. Moreover, we evaluate the quality of video reconstruction for each method by analyzing its performance in downstream tasks, including camera calibration, image classification, and object detection. Overall, we believe that EVREAL will contribute to the development of more effective and robust event-based video reconstruction methods. </p>
            </div>
            <div class="section">
                <h4 class="title">
                    News
                </h4>
             </div>
            <div class="content">
                <ol>
                    <li>In our <a href=https://ercanburak-evreal.hf.space/>result analysis tool</a>, we also share results of a new event-based video reconstruction model, <strong>HyperE2VID</strong>, which generates higher-quality videos than previous state-of-the-art, while also reducing memory consumption and inference time. Please see the <a href=https://ercanburak.github.io/HyperE2VID.html>HyperE2VID webpage</a> for more details.</li>
                    <li>The web application of our result analysis tool is ready now. <a href=https://ercanburak-evreal.hf.space/>Try it here</a> to interactively visualize and compare qualitative and quantitative results of event-based video reconstruction methods.</li>
                    <li>We will present our work at the CVPR Workshop on Event-Based Vision in person, on the 19th of June 2023, during Session 2 (starting at 10:30 local time). Please see the <a href=https://tub-rip.github.io/eventvision2023/>workshop website</a> for details.</li>
                </ol>
            </div>
            <div class="section">
                <h4 class="title">
                    Results
                </h4>
             </div>
            <div>
                <h3>Web app of result analysis tool to interactively visualize and compare results of event-based video reconstruction methods:</h3>
            </div>
            <div class="content">
                <iframe
                    src="https://ercanburak-evreal.hf.space/?header=false"
                    frameborder="0"
                    width="100%"
                    height="650"
                ></iframe>
            </div>
                <h3>Full-reference quantitative results on the ECD, MVSEC, HQF, and BS-ERGB datasets:</h3>
                <img src="projects/evreal/table_fullref.png" style="width:100%;">
                <br>
                <h3>No-reference quantitative results on challenging sequences involving fast motion, low light, and high-dynamic range:</h3>
                <img src="projects/evreal/table_noref.png" style="width:100%;">
                <br>
                <h3>Quantitative results on downstream tasks:</h3>
                <img src="projects/evreal/table_downstream.png" align="left" style="width:55%;">
            <div class="section">
                <h4 class="title">
                    BibTeX
                </h4>
                <p class="content">
                    <pre><code>@inproceedings{ercan2023evreal,
title={{EVREAL}: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction},
author={Ercan, Burak and Eker, Onur and Erdem, Aykut and Erdem, Erkut},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month={June},
year={2023},
pages={3942-3951}}</code></pre>
                </p>
            </div>
            <div class="section-title"> Acknowledgements </div>
            <div class="content">
                This work was supported in part by KUIS AI Center Research Award, TUBITAK-1001 Program Award No. 121E454, and BAGEP 2021 Award of the Science Academy to A. Erdem.
            </div>
        </div>
    </body>
</html>
