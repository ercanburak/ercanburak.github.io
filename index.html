<!Doctype html>
<html lang="en">
    <head>
        <title>Burak Ercan</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Burak Ercan">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div class="header">
            <div class="section">
                <div class="myname">Burak Ercan</div>
            </div>
        </div>
        <div class="section">
            <div class="row">
                <div class="image" id="profile-photo">
                    <img alt="Profile photo" src="figures/me-small.jpg" />
                    </a>
                </div>
                <div class="content">
                    <p>
                        Seasoned computer vision & AI engineer with a Ph.D. in <a href=https://arxiv.org/abs/1904.08405>event-based vision</a>, 
                        and 15+ years of industrial experience at large defense companies and AI startups. Proven track record in designing and implementing 
                        AI-driven solutions across diverse sectors, including defense, aviation, robotics, smart cities, transportation, and healthcare.
                    </p>
                    <p style="text-align:left; margin-top: 10px;">
                        <a href="docs/Burak_Ercan_CV.pdf">CV</a>
                        &nbsp;&middot;&nbsp;
                        <a href="docs/Burak_Ercan_PhD_Thesis_final.pdf">PhD Thesis</a>
                    </p>
                    <p style="text-align:center">
                        <span class="icon">
                            <a href=mailto:bercan89@gmail.com>
                                <i class="fa fa-envelope fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://scholar.google.com.tr/citations?hl=en&user=XkNBtssAAAAJ>
                            <i class="ai ai-google-scholar ai-3x"></i>
                            </a> 
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://github.com/ercanburak>
                               <i class="fa-brands fa-github fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://www.linkedin.com/in/burak-ercan/>
                               <i class="fa-brands fa-linkedin fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://x.com/ercanbur>
                                <i class="fa-brands fa-x-twitter fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                    </p>
                </div>
            </div>
            <!---
            <div class="title">
                <span>News</span>
            </div>
            <ul class=news_item>
            </ul>-->
            <div class="title">
                <span>Selected Publications</span>
            </div>
            <!-- start publication list --><div class="row paper"><div class="image"><img src="projects/HyperE2VID/detailed.png" alt="HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks" /></div><div class="content"><div class="paper-title"><a href="https://ercanburak.github.io/HyperE2VID.html">HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks</a></div><div class="conference">IEEE Transactions on Image Processing, 2024</div><div class="authors"><a href="" class="author">Burak Ercan</a>, <a href="https://github.com/ekeronur/" class="author">Onur Eker</a>, <a href="https://github.com/CanberkSaglam/" class="author">Canberk Saglam</a>, <a href="https://aykuterdem.github.io/" class="author">Aykut Erdem</a>, <a href="https://web.cs.hacettepe.edu.tr/~erkut/" class="author">Erkut Erdem</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="HyperE2VID.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2305.06382.pdf" data-type="Paper">Paper</a> <a href="https://github.com/ercanburak/HyperE2VID" data-type="Code">Code</a> <a href="https://www.youtube.com/watch?v=BWEV56-E0mE" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="5">Bibtex</a><div class="link-content" data-index="0">Event-based cameras are becoming increasingly popular for their ability to capture high-speed motion with low latency and high dynamic range. However, generating videos from events remains challenging due to the highly sparse and varying nature of event data. To address this, in this study, we propose HyperE2VID, a dynamic neural network architecture for event-based video reconstruction. Our approach uses hypernetworks to generate per-pixel adaptive filters guided by a context fusion module that combines information from event voxel grids and previously reconstructed intensity images. We also employ a curriculum learning strategy to train the network more robustly. Our comprehensive experimental evaluations across various benchmark datasets reveal that HyperE2VID not only surpasses current state-of-the-art methods in terms of reconstruction quality but also achieves this with fewer parameters, reduced computational requirements, and accelerated inference times.</div><div class="link-content" data-index="5"><pre>@article{ercan2024hypere2vid,
  title={{HyperE2VID}: Improving Event-Based Video Reconstruction via Hypernetworks},
  author={Ercan, Burak and Eker, Onur and Saglam, Canberk and Erdem, Aykut and Erdem, Erkut},
  journal={IEEE Transactions on Image Processing},
  year={2024},
  volume={33},
  pages={1826--1837},
  doi={10.1109/TIP.2024.3372460},
  publisher={IEEE}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="projects/evreal/diagram.png" alt="EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction" /></div><div class="content"><div class="paper-title"><a href="https://ercanburak.github.io/evreal.html">EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR) Workshops, 2023</div><div class="authors"><a href="" class="author">Burak Ercan</a>, <a href="https://github.com/ekeronur/" class="author">Onur Eker</a>, <a href="https://aykuterdem.github.io/" class="author">Aykut Erdem</a>, <a href="https://web.cs.hacettepe.edu.tr/~erkut/" class="author">Erkut Erdem</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="evreal.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2305.00434.pdf" data-type="Paper">Paper</a> <a href="https://github.com/ercanburak/EVREAL" data-type="Code">Code</a> <a href="https://ercanburak-evreal.hf.space/" data-type="Demo">Demo</a> <a href="projects/evreal/poster.pdf" data-type="Poster">Poster</a> <a href="#" data-type="Bibtex" data-index="6">Bibtex</a><div class="link-content" data-index="0">Event cameras are a new type of vision sensor that incorporates asynchronous and independent pixels, offering advantages over traditional frame-based cameras such as high dynamic range and minimal motion blur. However, their output is not easily understandable by humans, making the reconstruction of intensity images from event streams a fundamental task in event-based vision. While recent deep learning-based methods have shown promise in video reconstruction from events, this problem is not completely solved yet. To facilitate comparison between different approaches, standardized evaluation protocols and diverse test datasets are essential. This paper proposes a unified evaluation methodology and introduces an open-source framework called EVREAL to comprehensively benchmark and analyze various event-based video reconstruction methods from the literature. Using EVREAL, we give a detailed analysis of the state-of-the-art methods for event-based video reconstruction, and provide valuable insights into the performance of these methods under varying settings, challenging scenarios, and downstream tasks.</div><div class="link-content" data-index="6"><pre>@inproceedings{ercan2023evreal,
  title={{EVREAL}: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction},
  author={Ercan, Burak and Eker, Onur and Erdem, Aykut and Erdem, Erkut},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3942--3951},
  year={2023}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="projects/HUE/table.png" alt="HUE Dataset: High-Resolution Event and Frame Sequences for Low-Light Vision" /></div><div class="content"><div class="paper-title"><a href="https://ercanburak.github.io/HUE.html">HUE Dataset: High-Resolution Event and Frame Sequences for Low-Light Vision</a></div><div class="conference">European Conference on Computer Vision (ECCV) Workshops, 2024</div><div class="authors"><a href="" class="author">Burak Ercan</a>, <a href="https://github.com/ekeronur/" class="author">Onur Eker</a>, <a href="https://aykuterdem.github.io/" class="author">Aykut Erdem</a>, <a href="https://web.cs.hacettepe.edu.tr/~erkut/" class="author">Erkut Erdem</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="HUE.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2410.19164" data-type="Paper">Paper</a> <a href="#" data-type="Bibtex" data-index="2">Bibtex</a><div class="link-content" data-index="0">Low-light environments pose significant challenges for image enhancement methods. To address these challenges, in this work, we introduce the HUE dataset, a comprehensive collection of high-resolution event and frame sequences captured in diverse and challenging low-light conditions. Our dataset includes 106 sequences, encompassing indoor, cityscape, twilight, night, driving, and controlled scenarios, each carefully recorded to address various illumination levels and dynamic ranges. Utilizing a hybrid RGB and event camera setup, we collect a dataset that combines high-resolution event data with complementary frame data. We employ both qualitative and quantitative evaluations using no-reference metrics to assess state-of-the-art low-light enhancement and event-based image reconstruction methods. Additionally, we evaluate these methods on a downstream object detection task. Our findings reveal that while event-based methods perform well in specific metrics, they may produce false positives in practical applications. This dataset and our comprehensive analysis provide valuable insights for future research in low-light vision and hybrid camera systems.</div><div class="link-content" data-index="2"><pre>@inproceedings{ercan2025hue,
  title={{HUE} Dataset: High-Resolution Event and Frame Sequences for Low-Light Vision},
  author={Ercan, Burak and Eker, Onur and Erdem, Aykut and Erdem, Erkut},
  booktitle={Computer Vision -- ECCV 2024 Workshops},
  year={2025},
  pages={174-191}
}</pre></div></div></div></div><!-- end publication list -->

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143288088-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-143288088-1');
        </script>
    </body>
</html>
