<!Doctype html>
<html lang="en">
    <head>
        <title>Burak Ercan</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Burak Ercan">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div class="header">
            <div class="section">
                <div class="myname">Burak Ercan</div>
            </div>
        </div>
        <div class="section">
            <div class="row">
                <div class="image" id="profile-photo">
                    <img alt="Profile photo" src="figures/me-small.jpg" />
                    </a>
                </div>
                <div class="content">
                    <p>
                        I am a senior computer vision engineer at <a href=https://www.havelsan.com.tr/en>HAVELSAN</a> 
                        and a Ph.D. candidate at <a href=https://hacettepe.edu.tr/english>Hacettepe University</a>,
                        working on  <a href=https://arxiv.org/abs/1904.08405>event-based vision</a> 
                        with <a href=https://web.cs.hacettepe.edu.tr/~erkut/>Assoc. Prof. Erkut Erdem</a> 
                        and <a href=https://aykuterdem.github.io/>Assoc. Prof. Aykut Erdem</a>.
                        <br>
                        <br>
                    </p>
                    <p style="text-align:center">
                        <span class="icon">
                            <a href=mailto:burakercan@hacettepe.edu.tr>
                                <i class="fa fa-envelope fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://scholar.google.com.tr/citations?hl=en&user=XkNBtssAAAAJ>
                            <i class="ai ai-google-scholar ai-3x"></i>
                            </a> 
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://github.com/ercanburak>
                               <i class="fa fa-github fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://www.linkedin.com/in/burak-ercan/>
                               <i class="fa fa-linkedin fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                        &nbsp;&nbsp;
                        <span class="icon">
                            <a href=https://twitter.com/ercanbur>
                                <i class="fa fa-twitter fa-3x" aria-hidden="true"></i>
                            </a>
                        </span>
                    </p>
                </div>
            </div>
            <!---
            <div class="title">
                <span>News</span>
            </div>
            <ul class=news_item>
            </ul>-->
            <div class="title">
                <span>Selected Publications</span>
            </div>
            <!-- start publication list --><div class="row paper"><div class="image"><img src="projects/HyperE2VID/detailed.png" alt="HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks" /></div><div class="content"><div class="paper-title"><a href="https://ercanburak.github.io/HyperE2VID.html">HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks</a></div><div class="conference">arXiv, 2023</div><div class="authors"><a href="" class="author">Burak Ercan</a>, <a href="https://github.com/ekeronur/" class="author">Onur Eker</a>, <a href="https://github.com/CanberkSaglam/" class="author">Canberk Saglam</a>, <a href="https://aykuterdem.github.io/" class="author">Aykut Erdem</a>, <a href="https://web.cs.hacettepe.edu.tr/~erkut/" class="author">Erkut Erdem</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="HyperE2VID.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2305.06382.pdf" data-type="Paper">Paper</a> <a href="https://github.com/ercanburak/HyperE2VID" data-type="Code">Code</a> <a href="https://www.youtube.com/watch?v=BWEV56-E0mE" data-type="Video">Video</a> <a href="#" data-type="Bibtex" data-index="5">Bibtex</a><div class="link-content" data-index="0">Event-based cameras are becoming increasingly popular for their ability to capture high-speed motion with low latency and high dynamic range. However, generating videos from events remains challenging due to the highly sparse and varying nature of event data. To address this, in this study, we propose HyperE2VID, a dynamic neural anetwork architecture for event-based video reconstruction. Our approach uses hypernetworks to generate per-pixel adaptive filters guided by a context fusion module that combines information from event voxel grids and previously reconstructed intensity images. We also employ a curriculum learning strategy to train the network more robustly. Our comprehensive experimental evaluations across various benchmark datasets reveal that HyperE2VID not only surpasses current state-of-the-art methods in terms of reconstruction quality but also achieves this with fewer parameters, reduced computational requirements, and accelerated inference times.</div><div class="link-content" data-index="5"><pre>@article{ercan2023hypere2vid,
  title={{HyperE2VID}: Improving Event-Based Video Reconstruction via Hypernetworks},
  author={Ercan, Burak and Eker, Onur and Saglam, Canberk and Erdem, Aykut and Erdem, Erkut},
  journal={arXiv preprint arXiv:2305.06382},
  year={2023}
}</pre></div></div></div></div><div class="row paper"><div class="image"><img src="projects/evreal/diagram.png" alt="EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction" /></div><div class="content"><div class="paper-title"><a href="https://ercanburak.github.io/evreal.html">EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction</a></div><div class="conference">Computer Vision and Pattern Recognition (CVPR) Workshops, 2023</div><div class="authors"><a href="" class="author">Burak Ercan</a>, <a href="https://github.com/ekeronur/" class="author">Onur Eker</a>, <a href="https://aykuterdem.github.io/" class="author">Aykut Erdem</a>, <a href="https://web.cs.hacettepe.edu.tr/~erkut/" class="author">Erkut Erdem</a></div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="evreal.html" data-type="Project page">Project page</a> <a href="https://arxiv.org/pdf/2305.00434.pdf" data-type="Paper">Paper</a> <a href="https://github.com/ercanburak/EVREAL" data-type="Code">Code</a> <a href="https://ercanburak-evreal.hf.space/" data-type="Demo">Demo</a> <a href="projects/evreal/poster.pdf" data-type="Poster">Poster</a> <a href="#" data-type="Bibtex" data-index="6">Bibtex</a><div class="link-content" data-index="0">Event cameras are a new type of vision sensor that incorporates asynchronous and independent pixels, offering advantages over traditional frame-based cameras such as high dynamic range and minimal motion blur. However, their output is not easily understandable by humans, making the reconstruction of intensity images from event streams a fundamental task in event-based vision. While recent deep learning-based methods have shown promise in video reconstruction from events, this problem is not completely solved yet. To facilitate comparison between different approaches, standardized evaluation protocols and diverse test datasets are essential. This paper proposes a unified evaluation methodology and introduces an open-source framework called EVREAL to comprehensively benchmark and analyze various event-based video reconstruction methods from the literature. Using EVREAL, we give a detailed analysis of the state-of-the-art methods for event-based video reconstruction, and provide valuable insights into the performance of these methods under varying settings, challenging scenarios, and downstream tasks.</div><div class="link-content" data-index="6"><pre>@inproceedings{ercan2023evreal,
  title={EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction},
  author={Ercan, Burak and Eker, Onur and Erdem, Aykut and Erdem, Erkut},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3942--3951},
  year={2023}
}</pre></div></div></div></div><!-- end publication list -->

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143288088-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-143288088-1');
        </script>
    </body>
</html>